Tekkr Full Stack Hiring Challenge

This repository contains my solution for the Tekkr Full Stack Hiring Challenge.
The goal of the project was to extend an existing full-stack application with an LLM-based chat system, persistent chat sessions, and an inline project plan preview.

# Tech Stack
Frontend

React + TypeScript

React Router

React Query (TanStack Query)

shadcn/ui components

Tailwind CSS

Backend

Fastify

TypeScript

LLM integration (via a provider abstraction)

# Features Implemented
1. LLM-based Chat

User messages are sent to the backend.

The backend forwards the conversation to an LLM and returns the response.

A loading indicator is shown while the LLM response is generated.

Errors are handled gracefully and displayed in the chat UI.

2. Multiple Chat Sessions

Users can create a new chat using the “New Chat” button.

Each chat appears in the sidebar.

Users can switch between chats at any time.

Chat names are simple and autogenerated.

3. Persistent Chat History

All chats and messages are stored in localStorage.

On page reload:

All previous chats are restored.

The previously selected chat remains selected.

No database is required (as per the task scope).

4. Inline Project Plan Preview

When the LLM returns a project plan, it is embedded inline in the chat message.

The project plan preview:

Supports expandable / collapsible sections

Displays workstreams and deliverables

Can appear in the middle of a message, not only at the beginning or end

The preview is rendered using a dedicated React component.

5. Clean Separation of Concerns

App.tsx handles routing and providers only.

All chat state and logic live in the HomePage.

UI components (sidebar, input box, message rendering) are kept stateless and reusable.

Backend logic is isolated from frontend UI concerns.

6. LLM Provider Abstraction

The backend uses a provider interface for LLM calls.

This makes it easy to replace or add providers (e.g. OpenAI, Gemini, Anthropic) without changing the rest of the system.

# Project Structure
.
├── server
│   ├── src
│   │   ├── routes
│   │   │   └── chat.ts        # Chat completion endpoint
│   │   ├── services
│   │   │   └── llm            # LLM provider abstraction
│   │   └── app.ts
│   └── package.json
│
├── web
│   ├── src
│   │   ├── api                # Backend API calls
│   │   ├── components         # UI components
│   │   ├── lib                # Types, chat store, helpers
│   │   ├── pages
│   │   │   └── home-page.tsx  # Main chat logic
│   │   └── App.tsx            # Router & providers
│   └── package.json
│
└── README.md

# Setup Instructions
Prerequisites

Node.js (>= 18 recommended)

npm

# Backend Setup
cd server
npm install
npm start
Backend runs on:
http://localhost:8000

# Frontend Setup
cd web
npm install
npm start
Frontend runs on:
http://localhost:3000

# Usage

Open the frontend in your browser.

Create a new chat or select an existing one from the sidebar.

Type a message and press Enter (or click Send).

While the response is generated, a loading indicator is shown.

Project plans returned by the LLM are rendered inline with expandable sections.

Reload the page — chats and messages are preserved.

# Notes & Design Decisions

In-memory persistence via localStorage was chosen to match the task’s scope.

No authentication is implemented, as only a single user is assumed.

The application is designed for clarity and maintainability over premature optimization.

Mobile responsiveness was intentionally kept out of scope.
